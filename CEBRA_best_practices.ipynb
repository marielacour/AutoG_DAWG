{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpIuUAuOXx77"
      },
      "source": [
        "# Best Practices for training CEBRA models\n",
        "\n",
        "We show you how to get started with CEBRA:\n",
        "\n",
        "- Define a CEBRA-Time model (we strongly recommend starting with this).\n",
        "- Load data.\n",
        "- Perform train/validation splits.\n",
        "- Train the model.\n",
        "- Check the loss functions.\n",
        "- Save the model & reload.\n",
        "- Transform the model on train/val.\n",
        "- Evaluate Goodness of Fit.\n",
        "- Visualize the embeddings.\n",
        "- Compute and display the Consistency between runs (n=10).\n",
        "- Run a (small) grid search for model parameters.\n",
        "\n",
        "Once you have a good CEBRA-Time model, then you can do hypothesis testing with CEBRA-Behavior:\n",
        "- Define a CEBRA-Behavior model.\n",
        "- as above, train, check, evaluate, transform, and test consistency.\n",
        "- Run controls with shuffled data - which is critical for label-guided embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KtNYohGcZJJC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8wexciDCXx79",
        "outputId": "61406c8f-279f-43a1-d03d-6ac2a22c9d5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cebra[datasets,integrations] in /usr/local/lib/python3.12/dist-packages (0.6.0a2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (1.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (1.26.4)\n",
            "Requirement already satisfied: literate-dataclasses in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (0.0.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (1.16.3)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (4.67.1)\n",
            "Requirement already satisfied: matplotlib<3.11 in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (3.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (2.32.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (3.15.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (2.2.2)\n",
            "Requirement already satisfied: hdf5storage in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (0.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (3.1.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (5.24.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (0.13.2)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (0.8.0)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (1.6.7)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from cebra[datasets,integrations]) (0.25.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11->cebra[datasets,integrations]) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->cebra[datasets,integrations]) (3.4.0)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from cvxpy->cebra[datasets,integrations]) (1.0.5)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy->cebra[datasets,integrations]) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.12/dist-packages (from cvxpy->cebra[datasets,integrations]) (3.2.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl->cebra[datasets,integrations]) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->cebra[datasets,integrations]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->cebra[datasets,integrations]) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->cebra[datasets,integrations]) (8.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->cebra[datasets,integrations]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->cebra[datasets,integrations]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->cebra[datasets,integrations]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->cebra[datasets,integrations]) (2025.10.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->cebra[datasets,integrations]) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->cebra[datasets,integrations]) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->cebra[datasets,integrations]) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->cebra[datasets,integrations]) (3.6.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy->cebra[datasets,integrations]) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib<3.11->cebra[datasets,integrations]) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->cebra[datasets,integrations]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->cebra[datasets,integrations]) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy->cebra[datasets,integrations]) (2.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre 'cebra[datasets,integrations]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7ZZbFJthXx79",
        "outputId": "83b14957-1e35-465d-f30c-7d649ab700b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'numpy.strings'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1010809118.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCEBRA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cebra/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mis_load_deeplabcut_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cebra/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# NOTE(stes): intentional ordering of imports to avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#             these imports will not be reordered by isort (see .isort.cfg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cebra/data/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcebra_data_assets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasking\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcebra_data_masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cebra/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \"\"\"\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontinuous\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscrete\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cebra/distributions/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcebra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cebra/io.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.strings'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cebra.datasets\n",
        "from cebra import CEBRA\n",
        "\n",
        "#for model saving:\n",
        "import os\n",
        "import tempfile\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set up a CEBRA model\n",
        "\n",
        "### Items to consider\n",
        "\n",
        "- We recommend starting with an unsupervised approach (CEBRA-Time).\n",
        "- We recommend starting with defaults, perform the sanity checks we suggest below, then performing a grid search if needed.\n",
        "- We are going to largely follow the recommendations from our [Quick Start scikit-learn API](https://cebra.ai/docs/usage.html#quick-start-scikit-learn-api-example)"
      ],
      "metadata": {
        "id": "l84j08MdY4B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define a CEBRA model\n",
        "cebra_model = CEBRA(\n",
        "    model_architecture=\"offset10-model\", #consider: \"offset10-model-mse\" if Euclidean\n",
        "    batch_size=512,\n",
        "    learning_rate=3e-4,\n",
        "    temperature=1.12,\n",
        "    max_iterations=5000, #we will sweep later; start with default\n",
        "    conditional='time', #for supervised, put 'time_delta', or 'delta'\n",
        "    output_dimension=3,\n",
        "    distance='cosine', #consider 'euclidean'; if you set this, output_dimension min=2\n",
        "    device=\"cuda_if_available\",\n",
        "    verbose=True,\n",
        "    time_offsets=10\n",
        ")"
      ],
      "metadata": {
        "id": "kBl9YJ3SZECg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sEFn-1eXx7-"
      },
      "source": [
        "## 2. Load the data\n",
        "\n",
        "- (or adapt and use your data)\n",
        "- We are going to use demo data. The data will be automatically downloaded into a `/data` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieRd3hWiXx7-"
      },
      "outputs": [],
      "source": [
        "#2. example data\n",
        "%mkdir data\n",
        "hippocampus_pos = cebra.datasets.init('rat-hippocampus-single-achilles')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjLq5jRwXx7_"
      },
      "source": [
        "### Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0OYgU8DXx7_"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(9,3), dpi=150)\n",
        "plt.subplots_adjust(wspace = 0.3)\n",
        "ax = plt.subplot(121)\n",
        "ax.imshow(hippocampus_pos.neural.numpy()[:1000].T, aspect = 'auto', cmap = 'Blues')\n",
        "plt.ylabel('Neuron #')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.xticks(np.linspace(0, 1000, 5), np.linspace(0, 0.025*1000, 5, dtype = int))\n",
        "\n",
        "ax2 = plt.subplot(122)\n",
        "ax2.scatter(np.arange(1000), hippocampus_pos.continuous_index[:1000, 0],\n",
        "            c=hippocampus_pos.continuous_index[:1000, 0], cmap='rainbow', s=1)\n",
        "\n",
        "plt.ylabel('Position [m]')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.xticks(np.linspace(0, 1000, 5), np.linspace(0, 0.025*1000, 5, dtype = int))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick test: Train CEBRA-Time on the full data (not train/validation yet)...\n",
        "\n",
        "- This is a rapid quick start, just training without labels on the full dataset on the model we set up above! Here, we should already see a nice structured embedding.\n",
        "- Note, the colors here are post-hoc applied; positional information was not used to train the model."
      ],
      "metadata": {
        "id": "5uMhImnro6UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit\n",
        "cebra_time_full_model = cebra_model.fit(hippocampus_pos.neural)\n",
        "# transform\n",
        "cebra_time_full = cebra_model.transform(hippocampus_pos.neural)\n",
        "# GoF\n",
        "gof_full = cebra.sklearn.metrics.goodness_of_fit_score(cebra_time_full_model, hippocampus_pos.neural)\n",
        "print(\" GoF in bits - full:\", gof_full)\n",
        "# plot embedding\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_time_full, embedding_labels=hippocampus_pos.continuous_index[:,0], title = \"CEBRA-Time (full)\", markersize=3, cmap = \"rainbow\")\n",
        "fig.show()\n",
        "# plot the loss curve\n",
        "ax = cebra.plot_loss(cebra_time_full_model)"
      ],
      "metadata": {
        "id": "F5VovGWOo2sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Create a Train/Validation Split\n",
        "\n",
        "- now that we know we get something decent (see structure, proper loss curve), we can properly test parameters."
      ],
      "metadata": {
        "id": "ztVR3dmRaX9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split data and labels (labels we use later!)\n",
        "from sklearn.model_selection import train_test_split\n",
        "split_idx = int(0.8 * len(hippocampus_pos.neural)) #suggest: 5%-20% depending on your dataset size\n",
        "\n",
        "train_data = hippocampus_pos.neural[:split_idx]\n",
        "valid_data = hippocampus_pos.neural[split_idx:]\n",
        "\n",
        "train_continuous_label = hippocampus_pos.continuous_index.numpy()[:split_idx]\n",
        "valid_continuous_label = hippocampus_pos.continuous_index.numpy()[split_idx:]\n"
      ],
      "metadata": {
        "id": "CnTVN0CH4a8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Fit the train split model"
      ],
      "metadata": {
        "id": "VxujiFg0uTtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cebra_train_model = cebra_model.fit(train_data)#, train_continuous_label)"
      ],
      "metadata": {
        "id": "VxAZmACQcLDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Save the model [optional]"
      ],
      "metadata": {
        "id": "v62tJ3GzuWvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_file = Path(tempfile.gettempdir(), 'cebra.pt')\n",
        "cebra_train_model.save(tmp_file)\n",
        "#reload\n",
        "cebra_train_model = cebra.CEBRA.load(tmp_file)"
      ],
      "metadata": {
        "id": "WEbUc-EQeKn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Compute (transform) the embedding on train and validation data"
      ],
      "metadata": {
        "id": "XwpJtU3Wua5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_embedding = cebra_train_model.transform(train_data)\n",
        "valid_embedding = cebra_train_model.transform(valid_data)"
      ],
      "metadata": {
        "id": "5x-S5-KwcjU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluate the Model\n",
        "- Plot the loss curve\n",
        "- We can also look at the Goodness of Fit this in bits vs. the infoNCE loss. [See more info here](https://cebra.ai/docs/api/sklearn/metrics.html#cebra.integrations.sklearn.metrics.goodness_of_fit_score)\n",
        " - ProTip: 0 bits would be a perfectly collapsed embedding. Note, using GoF on the validation set is prone to low data regime issues, hence one should use the train loss to evaluate the model."
      ],
      "metadata": {
        "id": "wDPmwOlruF0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gof_train = cebra.sklearn.metrics.goodness_of_fit_score(cebra_train_model, train_data)\n",
        "print(\" GoF bits - train:\", gof_train)\n",
        "\n",
        "# plot the loss curve\n",
        "ax = cebra.plot_loss(cebra_train_model)"
      ],
      "metadata": {
        "id": "eeZcdy2udA1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the embeddings\n",
        "\n",
        "- train, then validation"
      ],
      "metadata": {
        "id": "YNe3i2izdWdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cebra.integrations.plotly\n",
        "#train\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(train_embedding,\n",
        "                                                           embedding_labels=train_continuous_label[:,0],\n",
        "                                                           title = \"CEBRA-Time Train\",\n",
        "                                                           markersize=3,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()\n",
        "\n",
        "#validation\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(valid_embedding,\n",
        "                                                           embedding_labels=valid_continuous_label[:,0],\n",
        "                                                           title = \"CEBRA-Time-validation\",\n",
        "                                                           markersize=3,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "H1pGQKN0dVn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next sanity/validation step: Consistency\n",
        "\n",
        "What did we check above?\n",
        " - (1) do we see structure in our embedding? (if not, something is off!)\n",
        " - (2) is the GoF reasonable? (infoNCE low, bits high)\n",
        " - (3) Is the loss converging without overfitting (no sudden drop after many interations?)\n",
        "\n",
        "IF 1-3 are not satisfactory, skip to the **Grid Search Below!**\n",
        "\n",
        "Beyond these being met, we need to check the consistency across runs! In addition to the above checks, once we have a converging model that produces **consistent embeddings**, then we know we have good model parameters! ðŸš€"
      ],
      "metadata": {
        "id": "1oG4sSqWgwK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Now we are going to run our train/val. 5-10 times to be sure they are consistent!\n",
        "\n",
        "X = 5  # Number of training runs\n",
        "model_paths = []  # Store file paths\n",
        "\n",
        "for i in range(X):\n",
        "    print(f\"Training ðŸ¦“CEBRA model {i+1}/{X}\")\n",
        "\n",
        "    # Train and save model\n",
        "    cebra_train_model = cebra_model.fit(train_data)\n",
        "    tmp_file = Path(tempfile.gettempdir(), f'cebra_{i}.pt')\n",
        "    cebra_train_model.save(tmp_file)\n",
        "    model_paths.append(tmp_file)\n",
        "\n",
        "### Reload models and transform data\n",
        "train_embeddings = []\n",
        "valid_embeddings = []\n",
        "\n",
        "for tmp_file in model_paths:\n",
        "    cebra_train_model = cebra.CEBRA.load(tmp_file)\n",
        "    train_embeddings.append(cebra_train_model.transform(train_data))\n",
        "    valid_embeddings.append(cebra_train_model.transform(valid_data))"
      ],
      "metadata": {
        "id": "qNIi__hk5whd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Consistency Across Runs\n",
        "- Now that we have 5-10 model runs, we can compute the consistency between runs.\n",
        "- TRAIN: This should be high (in the 90's on the train embeddings)! If not, in this demo, we simply suggest training slightly longer.\n",
        "- VALID: Depending on how large your validation data are, this also should be as high.\n",
        " - In our demo data, the cebra-time on rat 1 with 20% held out is in the 70's for 5K iterations, which is acceptable. One could consider training for longer (~8-9K)."
      ],
      "metadata": {
        "id": "gOE_H9EO8wrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores, pairs, ids_runs = cebra.sklearn.metrics.consistency_score(\n",
        "    embeddings=train_embeddings,\n",
        "    between=\"runs\"\n",
        ")\n",
        "\n",
        "cebra.plot_consistency(scores, pairs, ids_runs)"
      ],
      "metadata": {
        "id": "Weq_UuSLzBRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores, pairs, ids_runs = cebra.sklearn.metrics.consistency_score(\n",
        "    embeddings=valid_embeddings,\n",
        "    between=\"runs\"\n",
        ")\n",
        "\n",
        "cebra.plot_consistency(scores, pairs, ids_runs)"
      ],
      "metadata": {
        "id": "CRAaIO0v9DrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if I don't have good parameters? Let's do a grid search..."
      ],
      "metadata": {
        "id": "pkrb6rhcdefw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%mkdir saved_models\n",
        "\n",
        "params_grid = dict(\n",
        "    output_dimension = [3, 6],\n",
        "    time_offsets = [5, 10],\n",
        "    model_architecture='offset10-model',\n",
        "    temperature_mode='constant',\n",
        "    temperature=[0.1, 1.0],\n",
        "    max_iterations=[5000],\n",
        "    device='cuda_if_available',\n",
        "    num_hidden_units = [32, 64],\n",
        "    verbose = True)\n",
        "\n",
        "datasets = {\"dataset1\": train_data}\n",
        "# run the grid search\n",
        "grid_search = cebra.grid_search.GridSearch()\n",
        "grid_search.fit_models(datasets, params=params_grid, models_dir=\"saved_models\")"
      ],
      "metadata": {
        "id": "J91lFqBkX0DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the results\n",
        "df_results = grid_search.get_df_results(models_dir=\"saved_models\")\n",
        "\n",
        "# Get the best model for a given dataset\n",
        "best_model, best_model_name = grid_search.get_best_model(dataset_name=\"dataset1\", models_dir=\"saved_models\")\n",
        "print(\"The best model is:\", best_model_name)"
      ],
      "metadata": {
        "id": "vv_KI-sTHCyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the top model âœ¨\n",
        "model_path = Path(\"/content/saved_models\") / f\"{best_model_name}.pt\"\n",
        "top_model = cebra.CEBRA.load(model_path)\n",
        "\n",
        "#transform:\n",
        "top_train_embedding = top_model.transform(train_data)\n",
        "top_valid_embedding = top_model.transform(valid_data)\n",
        "\n",
        "# plot the loss curve\n",
        "ax = cebra.plot_loss(top_model)\n",
        "\n",
        "\n",
        "# plot embeddings\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(top_train_embedding,\n",
        "                                                           embedding_labels=train_continuous_label[:,0],\n",
        "                                                           title = \"top model - train\",\n",
        "                                                           markersize=3,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()\n",
        "\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(top_valid_embedding,\n",
        "                                                           embedding_labels=valid_continuous_label[:,0],\n",
        "                                                           title = \"top model - validation\",\n",
        "                                                           markersize=3,\n",
        "                                                           cmap = \"rainbow\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "s_ovXYbqN83q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CEBRA-Behavior: using auxiliary labels for hypothesis testing\n",
        "\n",
        "- Now that you have good parameters for a self-supervised embedding, the next goal is to understand which behavioral labels are contributing to the model fit.\n",
        "- Thus, we will use labels, such as position, for testing.\n",
        "- âš ï¸ We test model consistency on train/validation splits.\n",
        "- Then, we perform shuffle controls."
      ],
      "metadata": {
        "id": "WUIJ1WqCTfpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "# consider changing based on search/results above\n",
        "cebra_behavior_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=3,\n",
        "                        max_iterations=5000,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta', #using labels\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)"
      ],
      "metadata": {
        "id": "PR6_dCbvT1cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit\n",
        "cebra_behavior_full_model = cebra_behavior_model.fit(hippocampus_pos.neural,hippocampus_pos.continuous_index.numpy())\n",
        "# transform\n",
        "cebra_behavior_full = cebra_behavior_full_model.transform(hippocampus_pos.neural)\n",
        "# GoF\n",
        "gof_full = cebra.sklearn.metrics.goodness_of_fit_score(cebra_behavior_full_model, hippocampus_pos.neural,hippocampus_pos.continuous_index.numpy())\n",
        "print(\" GoF in bits - full:\", gof_full)\n",
        "# plot embedding\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_behavior_full, embedding_labels=hippocampus_pos.continuous_index[:,0], title = \"CEBRA-Behavior (full)\", markersize=3, cmap = \"rainbow\")\n",
        "fig.show()\n",
        "# plot the loss curve\n",
        "ax = cebra.plot_loss(cebra_behavior_full_model)"
      ],
      "metadata": {
        "id": "QDaOWmTaVppC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model consistency\n",
        "- run train/valid."
      ],
      "metadata": {
        "id": "BGGAnRpOjrdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are going to run our train/val. 5-10 times to be sure they are consistent!\n",
        "\n",
        "X = 5  # Number of training runs\n",
        "model_paths = []  # Store file paths\n",
        "\n",
        "for i in range(X):\n",
        "    print(f\"Training ðŸ¦“CEBRA model {i+1}/{X}\")\n",
        "\n",
        "    # Train and save model\n",
        "    cebra_behavior_train_model = cebra_behavior_model.fit(train_data,train_continuous_label)\n",
        "    tmp_file2 = Path(tempfile.gettempdir(), f'cebra_behavior_{i}.pt')\n",
        "    cebra_behavior_train_model.save(tmp_file2)\n",
        "    model_paths.append(tmp_file2)\n",
        "\n",
        "# Reload models and transform data\n",
        "train_behavior_embeddings = []\n",
        "valid_behavior_embeddings = []\n",
        "\n",
        "for tmp_file2 in model_paths:\n",
        "    cebra_behavior_train_model = cebra.CEBRA.load(tmp_file2)\n",
        "    train_behavior_embeddings.append(cebra_behavior_train_model.transform(train_data))\n",
        "    valid_behavior_embeddings.append(cebra_behavior_train_model.transform(valid_data))"
      ],
      "metadata": {
        "id": "flcUuUeEj0Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gof_train = cebra.sklearn.metrics.goodness_of_fit_score(cebra_behavior_train_model, train_data, train_continuous_label)\n",
        "print(\" GoF bits - train:\", gof_train)\n",
        "\n",
        "ax = cebra.plot_loss(cebra_behavior_train_model)"
      ],
      "metadata": {
        "id": "FJffzzKlMY1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "scores, pairs, ids_runs = cebra.sklearn.metrics.consistency_score(\n",
        "    embeddings=train_behavior_embeddings,\n",
        "    between=\"runs\"\n",
        ")\n",
        "\n",
        "cebra.plot_consistency(scores, pairs, ids_runs)\n",
        "\n",
        "#validation\n",
        "scores, pairs, ids_runs = cebra.sklearn.metrics.consistency_score(\n",
        "    embeddings=valid_behavior_embeddings,\n",
        "    between=\"runs\"\n",
        ")\n",
        "\n",
        "cebra.plot_consistency(scores, pairs, ids_runs)"
      ],
      "metadata": {
        "id": "WhMj8cKckndu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The next item to do when using labels is to perform shuffle controls\n",
        "\n",
        "**Why do we do this?**\n",
        "It is entirely expected that the distribution of the labels shape the embedding (see [Proposition 7, Supplementary Note 2](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-023-06031-6/MediaObjects/41586_2023_6031_MOESM1_ESM.pdf)). In [Figure 2c](https://www.nature.com/articles/s41586-023-06031-6#Fig2) we therefore show shuffle controls should be performed, and demonstrate that if **labels are shuffled**, it is not possible to fit them.\n",
        "\n",
        "- In our paper we shuffle the labels across time. When the `time_delta` strategy for positive sampling is used, this changes the distribution of the positive samples. We do this kind of control to show that fitting a model on nonsensical label structure (in a real experiment, this would be a behavior time series without connection to the neural data) is not possible (assuming a sufficiently large dataset).\n",
        "\n",
        "Another approach is to shuffle the neural data. However, if the model has sufficient capacity (is large), the data is limited, and one trains too long, it *can* fit a â€œlookup tableâ€ from input data to the output embedding to match the label distribution (because this is intact and still has structure). Thus, this can be useful to be sure you are not overparameterizing or training too long!\n",
        "\n",
        "- If you shuffle the neural data, the question is whether the label structure can be forced on a latent representation of the shuffled data. This will be possible as long as the model has enough capacity to fit a lookup table, where for each timepoint the embedding is arranged to fit the behavior."
      ],
      "metadata": {
        "id": "Qo6iYXylRct4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label-Shuffle Control"
      ],
      "metadata": {
        "id": "GS5uCmRXZWFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Shuffle control model:\n",
        "cebra_shuffled_model = CEBRA(model_architecture='offset10-model',\n",
        "                        batch_size=512,\n",
        "                        learning_rate=3e-4,\n",
        "                        temperature=1,\n",
        "                        output_dimension=3,\n",
        "                        max_iterations=5000,\n",
        "                        distance='cosine',\n",
        "                        conditional='time_delta',\n",
        "                        device='cuda_if_available',\n",
        "                        verbose=True,\n",
        "                        time_offsets=10)"
      ],
      "metadata": {
        "id": "WG4XiHOqS3Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the behavior variable and use it for training\n",
        "shuffled_pos = np.random.permutation(hippocampus_pos.continuous_index[:,0])"
      ],
      "metadata": {
        "id": "I9kyCxjfWchc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit, transform\n",
        "cebra_shuffled_model.fit(hippocampus_pos.neural, shuffled_pos)\n",
        "cebra_pos_shuffled = cebra_shuffled_model.transform(hippocampus_pos.neural)\n",
        "# GoF\n",
        "gof_full = cebra.sklearn.metrics.goodness_of_fit_score(cebra_shuffled_model, hippocampus_pos.neural,hippocampus_pos.continuous_index.numpy())\n",
        "print(\" GoF in bits - full:\", gof_full)\n",
        "# plot embedding\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_pos_shuffled, embedding_labels=hippocampus_pos.continuous_index[:,0], title = \"CEBRA-Behavior (labels shuffled)\", markersize=3, cmap = \"rainbow\")\n",
        "fig.show()\n",
        "# plot the loss curve\n",
        "ax = cebra.plot_loss(cebra_shuffled_model)"
      ],
      "metadata": {
        "id": "YpWJJN35YEIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Shuffle Control"
      ],
      "metadata": {
        "id": "r4FvnWVXZaj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Shuffle the neural data and use it for training\n",
        "shuffle_idx = np.random.permutation(len(hippocampus_pos.neural))\n",
        "shuffled_neural = hippocampus_pos.neural[shuffle_idx]"
      ],
      "metadata": {
        "id": "6E1CRPMsZcYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit, transform\n",
        "cebra_shuffled_model.fit(shuffled_neural, hippocampus_pos.continuous_index.numpy())\n",
        "cebra_neural_shuffled = cebra_shuffled_model.transform(shuffled_neural)\n",
        "# GoF\n",
        "gof_full = cebra.sklearn.metrics.goodness_of_fit_score(cebra_shuffled_model, shuffled_neural,hippocampus_pos.continuous_index.numpy())\n",
        "print(\" GoF in bits - full:\", gof_full)\n",
        "# plot embedding\n",
        "fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_neural_shuffled, embedding_labels=hippocampus_pos.continuous_index[:,0], title = \"CEBRA-Behavior (neural shuffled)\", markersize=3, cmap = \"rainbow\")\n",
        "fig.show()\n",
        "# plot the loss curve\n",
        "ax = cebra.plot_loss(cebra_shuffled_model)"
      ],
      "metadata": {
        "id": "puQTsMIzZnG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where do you land? ðŸš¨\n",
        "\n",
        "The shuffles with the same parameters should not show any structure, the GoF close to 0, and the loss curve should not drop late in training (which would be overfitting).\n",
        "\n",
        "If this is the case, then you have good parameters to go forth with! ðŸ¦“ðŸ¾\n",
        "\n",
        "### What's next?\n",
        "\n",
        "We recommend using these embeddings for model comparisons, decoding, explainable AI (xCEBRA), and/or representation analysis!"
      ],
      "metadata": {
        "id": "UVNTjeWRY7xa"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "nbsphinx": {
      "orphan": true
    },
    "vscode": {
      "interpreter": {
        "hash": "d20c091758da15a81f3fc9819bfd09a9744e7c7bb5bce53ce9fffdcf0f66921a"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}